<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="We propose learning 3D representations from procedurally generated shapes, matching state-of-the-art models trained on semantic data. Our approach highlights SSL's strength in capturing geometric structure over high-level semantics in 3D tasks.">
  <meta name="keywords" content="Self-Supervised Learning, 3D Representation Learning, Procedural 3D Programs">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Learning 3D Representations from Procedural 3D Programs</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Learning 3D Representations from Procedural 3D Programs</h1>
          <div class="is-size-5 publication-authors">
            <div class="is-size-5 publication-authors">
              <a href="https://xuweiyichen.github.io/" class="author-block" style="margin-right: 15px; text-decoration: none;">
                <span>Xuweiyi Chen</span>
              </a>
              <a href="https://sites.google.com/site/zezhoucheng/" class="author-block" style="text-decoration: none;">
                <span>Zezhou Cheng</span>
              </a>
            </div>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">University of Virginia</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
                <span class="link-block">
                  <a href=""
                      class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        &#129303; 
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>GitHub</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="tldr-section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h2 class="title is-4">TL;DR</h2>
        <p class="is-size-5">
          Training from procedural 3D programs performs on par with training from ShapeNet on shape classification and part segmentation tasks, and can reconstruct masked ShapeNet point cloud without fine-tuning.
        </p>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <figure>
      <img src="./static/images/teaser-v3.jpg" alt="Comparison of ROPE with Existing Benchmarks">
      <figcaption>
        <p><strong>Key Insights and Findings:</strong> 
          (a) Point-MAE-SN is trained on ShapeNet, providing semantically meaningful 3D models. 
          (b) Point-MAE-Zero is trained on procedurally generated 3D shapes without semantic structure. 
          (c) Point-MAE-Zero matches or outperforms Point-MAE-SN on tasks like ModelNet40, ScanObjectNN, and ShapeNetPart, significantly outperforming training from scratch.
        </p>
      </figcaption>
    </figure>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Self-supervised learning has emerged as a promising approach for acquiring transferable 3D representations from unlabeled 3D point clouds. Unlike 2D images, which are widely accessible, acquiring 3D assets requires specialized expertise or professional 3D scanning equipment, making it difficult to scale and raising copyright concerns. To address these challenges, we propose learning 3D representations from procedural 3D programs that automatically generate 3D shapes using simple primitives and augmentations. Remarkably, despite lacking semantic content, the 3D representations learned from this synthesized dataset perform on par with state-of-the-art representations learned from semantically recognizable 3D models (e.g., airplanes) across various downstream 3D tasks, including shape classification, part segmentation, and masked point cloud completion. Our analysis further suggests that current self-supervised learning methods primarily capture geometric structures rather than high-level semantics.          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Point-MAE-Zero</h2>
    <div class="content has-text-justified">
      <p>
        <strong>Point-MAE-Zero</strong> is a self-supervised framework for learning 3D representations entirely from procedurally generated shapes, eliminating reliance on human-designed 3D models. Based on the Point-MAE architecture, it employs a masked autoencoding scheme, where 60% of input point patches are masked and reconstructed using a transformer-based encoder-decoder. The reconstruction loss is computed via the Chamfer Distance between predicted and ground-truth point patches. This approach demonstrates the potential of procedural generation for 3D representation learning, with zero human involvement beyond the initial programming.
      </p>
      <figure>
        <img src="./static/images/pipeline.jpg" alt="Point-MAE-Zero Pipeline">
        <figcaption>
          <div class="content">
            <p><strong>(a)</strong> Our synthetic 3D point clouds are generated by sampling, compositing, and augmenting simple primitives with procedural 3D programs. <strong>(b)</strong> We use Point-MAE as our pretraining framework to learn 3D representation from synthetic 3D shapes, dubbed <strong><em>Point-MAE-Zero</em></strong> where "Zero" underscores that we do not use any human-made 3D shapes. <strong>(c)</strong> We evaluate <strong><em>Point-MAE-Zero</em></strong> in various 3D shape understanding tasks.</p>
          </div>
        </figcaption>
      </figure>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Masked Point Cloud Completion</h2>
    <div class="content has-text-justified">
      <p>
        The goal of masked point cloud completion is to reconstruct masked points in 3D point clouds, serving as a pretext task for learning 3D representations. During pretraining, a portion of point patches (e.g., 60%) is randomly masked, with only visible patches passed to the encoder, while masked patch centers guide the decoder. After pretraining, the network can reconstruct missing points with or without guidance. Experiments on ShapeNet and procedurally generated 3D shapes show that <strong>Point-MAE-Zero</strong>, trained solely on procedurally generated data, performs comparably to Point-MAE-SN on both datasets. Both models effectively leverage symmetry to estimate missing parts and exhibit slightly better performance on in-domain data. Performance declines when guidance is removed, but representations learned through masked autoencoding capture geometric features rather than semantic content.
      </p>
      <figure>
        <img src="./static/images/shape_recon.jpg" alt="Masked Point Cloud Completion with two settings">
        <figcaption>
          <div class="content">
            <p><strong>Heterogeneous ROPE sample.</strong> This figure visualizes shape completion results with Point-MAE-SN and Point-MAE-Zero on the test split of ShapeNet and procedurally synthesized 3D shapes. 
            <strong>Left:</strong> Ground truth 3D point clouds and masked inputs with a 60% mask ratio. 
            <strong>Middle:</strong> Shape completion results using the centers of masked input patches as guidance, following the training setup of Point-MAE. 
            <strong>Right:</strong> Point cloud reconstructions without any guidance points. 
            The $L_2$ Chamfer distance (<em>lower is better</em>) between the predicted 3D point clouds and the ground truth is displayed below each reconstruction.
            </p>
          </div>
        </figcaption>
      </figure>
    </div>

    <!-- Visualization Table -->
    <h2 class="title is-4 has-text-centered">Additional Visualizations</h2>
    <table class="table is-striped is-hoverable is-fullwidth">
      <thead>
        <tr>
          <th>Visualization Type</th>
          <th>Description</th>
          <th>Link</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Point-MAE-SN on ShapeNet</td>
          <td>Visualization of Point-MAE-SN on ShapeNet.</td>
          <td><a href="./static/images/visualization_results_shapenet_on_shapenet_2.html" class="button is-link">View</a></td>
        </tr>
        <tr>
          <td>Point-MAE-SN on ShapeNet (No guidance points)</td>
          <td>Visualization without guidance points.</td>
          <td><a href="./static/images/visualization_results_shapenet_on_shapenet_no_mask_2.html" class="button is-link">View</a></td>
        </tr>
        <tr>
          <td>Point-MAE-Zero on Synthetic Data</td>
          <td>Visualization of Point-MAE-Zero on synthetic data.</td>
          <td><a href="./static/images/visualization_results_zeroverse_on_zeroverse_2.html" class="button is-link">View</a></td>
        </tr>
        <tr>
          <td>Point-MAE-Zero on Synthetic Data (No guidance points)</td>
          <td>Visualization without guidance points.</td>
          <td><a href="./static/images/visualization_results_zeroverse_on_zeroverse_no_mask_2.html" class="button is-link">View</a></td>
        </tr>
        <tr>
          <td>Point-MAE-SN on Synthetic Data</td>
          <td>Visualization of Point-MAE-SN on synthetic data.</td>
          <td><a href="./static/images/visualization_results_shapenet_on_zeroverse_2.html" class="button is-link">View</a></td>
        </tr>
        <tr>
          <td>Point-MAE-SN on Synthetic Data (No guidance points)</td>
          <td>Visualization without guidance points.</td>
          <td><a href="./static/images/visualization_results_shapenet_on_zeroverse_no_mask_2.html" class="button is-link">View</a></td>
        </tr>
        <tr>
          <td>Point-MAE-Zero on ShapeNet</td>
          <td>Visualization of Point-MAE-Zero on ShapeNet.</td>
          <td><a href="./static/images/visualization_results_zeroverse_on_shapenet_2.html" class="button is-link">View</a></td>
        </tr>
        <tr>
          <td>Point-MAE-Zero on ShapeNet (No guidance points)</td>
          <td>Visualization without guidance points.</td>
          <td><a href="./static/images/visualization_results_zeroverse_on_shapenet_no_mask_2.html" class="button is-link">View</a></td>
        </tr>
      </tbody>
    </table>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Object Classification Results</h2>
    <div class="content has-text-justified">
      <p>
        In object classification, Point-MAE-Zero achieves performance comparable to Point-MAE-SN on ModelNet40, highlighting the impact of domain differences between procedurally generated shapes and clean 3D models. On ScanObjectNN, Point-MAE-Zero outperforms Point-MAE-SN across all variants, demonstrating the benefits of pretraining on diverse and procedurally synthesized 3D shapes. Both models surpass training from scratch and other self-supervised approaches. Please see our paper for more results.
      </p>
      <figure>
        <img src="/Users/chenxuweiyi/Desktop/point-mae-zero/static/images/object_classification_comparison.png" alt="Object Classification Comparison">
        <figcaption>
          <div class="content">
            <p>Object classification results comparing Point-MAE-Zero and Point-MAE-SN across various benchmarks.</p>
          </div>
        </figcaption>
      </figure>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Efficiency of Transfer Learning</h2>
    <div class="content has-text-justified">
      <p>
        The learning curves highlight the efficiency of transfer learning with Point-MAE-SN and Point-MAE-Zero. Both models converge faster and achieve higher test accuracy compared to training from scratch, a trend observed across ModelNet40 and ScanObjectNN benchmarks.
      </p>
      <figure>
        <img src="/Users/chenxuweiyi/Desktop/point-mae-zero/static/images/learning_curve_updated.jpg" alt="Transfer Learning Efficiency">
        <figcaption>
          <div class="content">
            <p>Learning curves for training from scratch, Point-MAE-SN, and Point-MAE-Zero on shape classification tasks.</p>
          </div>
        </figcaption>
      </figure>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">t-SNE Visualization</h2>
    <div class="content has-text-justified">
      <p>
        The t-SNE visualization compares 3D shape representations from Point-MAE-SN and Point-MAE-Zero, highlighting differences before and after fine-tuning. Before fine-tuning, both models show improved separation of categories compared to training from scratch, demonstrating the effectiveness of self-supervised pretraining. After fine-tuning, the separation becomes less distinct, suggesting that high-level semantic features may not be fully learned through the masked autoencoding scheme. Structural similarities between Point-MAE-SN and Point-MAE-Zero representations suggest that both models capture comparable 3D features despite pretraining on different datasets.
      </p>
      <figure>
        <img src="/Users/chenxuweiyi/Desktop/point-mae-zero/static/images/tsne.jpg" alt="t-SNE Visualization">
        <figcaption>
          <div class="content">
            <p>t-SNE visualization of 3D shape representations from Point-MAE-SN and Point-MAE-Zero before and after fine-tuning.</p>
          </div>
        </figcaption>
      </figure>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Citation</h2>
    <pre><code>
  </code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/sled-group/multi-object-hallucination" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed
            under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
